{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining project: Discover and describe areas of interest an events from geo-located data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #1: Import Dataset and Librarie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of required libraries and dependencies\n",
    "# numeric calculations\n",
    "! pip install numpy==1.26.0 \n",
    "# data frames \n",
    "! pip install pandas==2.1.1 \n",
    "# machine learning algorithms \n",
    "! pip install scikit-learn==1.5.1 \n",
    "! pip install scipy==1.12.0\n",
    "# plotting \n",
    "! pip install plotly==5.24.1 \n",
    "! pip install matplotlib==3.8.0 \n",
    "! pip install seaborn==0.13.2 \n",
    "! pip install plotly-express==0.4.1 \n",
    "! pip install chart-studio==1.1.0 \n",
    "# web app library \n",
    "! pip install streamlit==1.37.1 \n",
    "# association rules\n",
    "! pip install mlxtend==0.23.3 \n",
    "! pip install folium==0.19.4\n",
    "! pip install geopy==2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas to deal with the data\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import ConvexHull, QhullError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier CSV\n",
    "data = pd.read_csv(\"../data/dataSet.csv\", sep=\",\", low_memory=False)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes 11 et 12 (indices 10 et 11 en pandas)\n",
    "columns_to_check = data.iloc[:, [11, 12]]\n",
    "\n",
    "# Parcourir chaque colonne sélectionnée\n",
    "for col in columns_to_check.columns:\n",
    "    print(f\"Analyse de la colonne: {col}\")\n",
    "    \n",
    "    # Trouver les types de données présents\n",
    "    types_present = data[col].map(type).value_counts()\n",
    "    print(\"Types présents dans cette colonne :\")\n",
    "    print(types_present)\n",
    "    \n",
    "    # Afficher l'utilisation de la mémoire pour cette colonne\n",
    "    memory_usage = data[col].memory_usage(deep=True)\n",
    "    print(f\"Utilisation de la mémoire pour cette colonne : {memory_usage} bytes\\n\")\n",
    "\n",
    "for col in columns_to_check.columns:\n",
    "    print(f\"Conversion de la colonne: {col}\")\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0)  # Convertir en float, remplacer les erreurs par NaN\n",
    "\n",
    "    # Vérifier le résultat\n",
    "    print(f\"Colonne {col} après conversion:\")\n",
    "    print(data[col].head())  # Afficher les premières lignes pour valider\n",
    "    print(f\"Types après conversion: {data[col].map(type).value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columms\n",
    "data.columns = data.columns.str.strip()\n",
    "print(\"Colonnes après nettoyage :\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial: {len(data)}\")\n",
    "print(data.columns)\n",
    "columns_to_clean = [\n",
    "\t\t\t\t\t'lat', 'long', 'date_taken_minute', 'date_taken_hour', \n",
    "\t\t\t\t\t'date_taken_day', 'date_taken_month', 'date_taken_year', \n",
    "\t\t\t\t\t'date_upload_minute', 'date_upload_hour', 'date_upload_day', \n",
    "\t\t\t\t\t'date_upload_month', 'date_upload_year'\n",
    "\t\t\t\t\t]\n",
    "data_cleaned = data.dropna(subset=columns_to_clean)\n",
    "print(f\"After removing missing values: {len(data_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any duplicates\n",
    "data_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates if necessary\n",
    "print(f\"Initial: {len(data_cleaned)}\")\n",
    "data_cleaned = data_cleaned.drop_duplicates(subset=['user','lat','long','title','date_taken_year','date_taken_month','date_taken_day'],keep='first')\n",
    "print(f\"After removing duplicates: {len(data_cleaned)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data with a non-null values at column 16 is sorted differently. We conclude that it is hard to determine which value belongs to which column since date_taken_minute, date_taken_month can be ambigous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18']\n",
    "column_data = data_cleaned.dropna(subset=columns, how='all')\n",
    "\n",
    "print(column_data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows that have non-null values in columns 16, 17 and 18 are retracted from the dataset. These columns thus serve no purpose and are removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to check for non-null values\n",
    "columns_to_check = ['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18']\n",
    "\n",
    "# Drop rows where any of the specified columns are not null\n",
    "print(f\"Initial: {len(data_cleaned)}\")\n",
    "data_cleaned = data_cleaned[data_cleaned[columns_to_check].isnull().all(axis=1)]\n",
    "print(f\"After removing non-null values: {len(data_cleaned)}\")\n",
    "\n",
    "# Drop the columns\n",
    "data_cleaned = data_cleaned.drop(columns=columns)\n",
    "\n",
    "print(data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.to_csv(\"../data/data-cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.describe(exclude=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocalisation Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=(45.764, 4.8357), zoom_start=12)\n",
    "\n",
    "# Sample a subset of the data (e.g., 500 points)\n",
    "sampled_data = data_cleaned.sample(n=1000, random_state=1)\n",
    "\n",
    "latitude_col = sampled_data['lat']\n",
    "longitude_col = sampled_data['long']\n",
    "\n",
    "for i in range(0, len(sampled_data)):\n",
    "\tfolium.Marker([latitude_col.iloc[i], longitude_col.iloc[i]]).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocalisation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "m = folium.Map(location=(45.764, 4.8357), zoom_start=12)\n",
    "\n",
    "# Sample a subset of the data (e.g., 500 points)\n",
    "sampled_data = data_cleaned.sample(n=1000, random_state=1)\n",
    "\n",
    "latitude_col = sampled_data['lat']\n",
    "longitude_col = sampled_data['long']\n",
    "\n",
    "heat_data = [[row['lat'], row['long']] for index, row in sampled_data.iterrows()]\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only data necessary to create clusters here would be the latitude and longitude. We'll drop all other columns as it is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to keep\n",
    "columns = ['lat', 'long']\n",
    "\n",
    "# Drop all columns except the specified ones\n",
    "df_clustering = data_cleaned[columns]\n",
    "print(df_clustering.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the longitude and latitude have comparable scales, we can use the StandardScaler to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_clustering)\n",
    "scaled_data_df = pd.DataFrame(data=scaled_data, columns=df_clustering.columns)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(scaled_data_df['lat'], scaled_data_df['long'], alpha=0.5)\n",
    "plt.title('Scaled Latitude and Longitude')\n",
    "plt.xlabel('Scaled Latitude')\n",
    "plt.ylabel('Scaled Longitude')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the same pattern observed in the heatmap is reproduced with the scaled data. This is expected, as the data has been proportionally scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find the optimal number of clusters using Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of k values to try\n",
    "k_values = range(1, 40)\n",
    "sum_square = []\n",
    "\n",
    "for k in k_values:\n",
    "\tkmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "\tkmeans.fit(scaled_data_df)\n",
    "\tsum_square.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_values, sum_square)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum square')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters can be interpreted to be between 6-10. We found that with 6-10 clusters, the granularity of places is not as well defined as the number of points are extremely vast and spreaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20, init='k-means++')\n",
    "kmeans.fit(scaled_data_df)\n",
    "data_cleaned['cluster_kmeans'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center map\n",
    "m = folium.Map(location=[data_cleaned['lat'].mean(), data_cleaned['long'].mean()], zoom_start=12)\n",
    "\n",
    "palette = sns.color_palette(\"hsv\", n_colors=data_cleaned['cluster_kmeans'].nunique())\n",
    "colors = [mcolors.rgb2hex(color) for color in palette]\n",
    "\n",
    "centroids = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "sampled_data = data_cleaned.sample(n=20000, random_state=1)\n",
    "\n",
    "# Filter points based on distance to centroids\n",
    "filtered_points = sampled_data[sampled_data.apply(\n",
    "    lambda row: any(geodesic((row['lat'], row['long']), centroid).m <= 200 for centroid in centroids), axis=1)]\n",
    "\n",
    "# Map markers\n",
    "for i, row in filtered_points.iterrows():\n",
    "    point = (row['lat'], row['long'])\n",
    "    folium.CircleMarker(\n",
    "        location=point,\n",
    "        radius=1,\n",
    "        color=colors[row['cluster_kmeans']],\n",
    "        fill=True,\n",
    "        fill_color=colors[row['cluster_kmeans']]\n",
    "    ).add_to(m)\n",
    "\n",
    "# Centroids and hulls\n",
    "for cluster_id in range(kmeans.n_clusters):\n",
    "    cluster_points = filtered_points[filtered_points['cluster_kmeans'] == cluster_id][['lat', 'long']].values\n",
    "    if len(cluster_points) >= 3:  # ConvexHull requires at least 3 points\n",
    "        try: \n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=colors[cluster_id],\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Skipping cluster {cluster_id} due to QhullError\")\n",
    "    centroid = centroids[cluster_id]\n",
    "    folium.Marker(\n",
    "        location=[centroid[0], centroid[1]],\n",
    "        popup=f'Centroid {cluster_id}',\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"../kmeans_map.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
