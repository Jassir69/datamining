{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining project: Discover and describe areas of interest an events from geo-located data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #1: Import Dataset and Librarie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of required libraries and dependencies\n",
    "# numeric calculations\n",
    "! pip install numpy==1.26.0 \n",
    "# data frames \n",
    "! pip install pandas==2.1.1 \n",
    "# machine learning algorithms \n",
    "! pip install scikit-learn==1.5.1 \n",
    "! pip install scipy==1.12.0\n",
    "# plotting \n",
    "! pip install plotly==5.24.1 \n",
    "! pip install matplotlib==3.8.0 \n",
    "! pip install seaborn==0.13.2 \n",
    "! pip install plotly-express==0.4.1 \n",
    "! pip install chart-studio==1.1.0 \n",
    "# web app library \n",
    "! pip install streamlit==1.37.1 \n",
    "# association rules\n",
    "! pip install mlxtend==0.23.3 \n",
    "! pip install folium==0.19.4\n",
    "! pip install geopy==2.4.1\n",
    "! pip install pyproj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas to deal with the data\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import ConvexHull, QhullError\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering,  DBSCAN  ,linkage_tree  \n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import folium\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier CSV\n",
    "data = pd.read_csv(\"../data/dataSet.csv\", sep=\",\", low_memory=False)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes 11 et 12 (indices 10 et 11 en pandas)\n",
    "columns_to_check = data.iloc[:, [11, 12]]\n",
    "\n",
    "# Parcourir chaque colonne sélectionnée\n",
    "for col in columns_to_check.columns:\n",
    "    print(f\"Analyse de la colonne: {col}\")\n",
    "    \n",
    "    # Trouver les types de données présents\n",
    "    types_present = data[col].map(type).value_counts()\n",
    "    print(\"Types présents dans cette colonne :\")\n",
    "    print(types_present)\n",
    "    \n",
    "    # Afficher l'utilisation de la mémoire pour cette colonne\n",
    "    memory_usage = data[col].memory_usage(deep=True)\n",
    "    print(f\"Utilisation de la mémoire pour cette colonne : {memory_usage} bytes\\n\")\n",
    "\n",
    "for col in columns_to_check.columns:\n",
    "    print(f\"Conversion de la colonne: {col}\")\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0)  # Convertir en float, remplacer les erreurs par NaN\n",
    "\n",
    "    # Vérifier le résultat\n",
    "    print(f\"Colonne {col} après conversion:\")\n",
    "    print(data[col].head())  # Afficher les premières lignes pour valider\n",
    "    print(f\"Types après conversion: {data[col].map(type).value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columms\n",
    "data.columns = data.columns.str.strip()\n",
    "print(\"Colonnes après nettoyage :\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial: {len(data)}\")\n",
    "print(data.columns)\n",
    "columns_to_clean = [\n",
    "\t\t\t\t\t'lat', 'long', 'date_taken_minute', 'date_taken_hour', \n",
    "\t\t\t\t\t'date_taken_day', 'date_taken_month', 'date_taken_year', \n",
    "\t\t\t\t\t'date_upload_minute', 'date_upload_hour', 'date_upload_day', \n",
    "\t\t\t\t\t'date_upload_month', 'date_upload_year'\n",
    "\t\t\t\t\t]\n",
    "data_cleaned = data.dropna(subset=columns_to_clean)\n",
    "print(f\"After removing missing values: {len(data_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any duplicates\n",
    "data_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates if necessary\n",
    "print(f\"Initial: {len(data_cleaned)}\")\n",
    "data_cleaned = data_cleaned.drop_duplicates(subset=['user','lat','long','title','date_taken_year','date_taken_month','date_taken_day'],keep='first')\n",
    "print(f\"After removing duplicates: {len(data_cleaned)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data with a non-null values at column 16 is sorted differently. We conclude that it is hard to determine which value belongs to which column since date_taken_minute, date_taken_month can be ambigous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18']\n",
    "column_data = data_cleaned.dropna(subset=columns, how='all')\n",
    "\n",
    "print(column_data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows that have non-null values in columns 16, 17 and 18 are retracted from the dataset. These columns thus serve no purpose and are removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to check for non-null values\n",
    "columns_to_check = ['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18']\n",
    "\n",
    "# Drop rows where any of the specified columns are not null\n",
    "print(f\"Initial: {len(data_cleaned)}\")\n",
    "data_cleaned = data_cleaned[data_cleaned[columns_to_check].isnull().all(axis=1)]\n",
    "print(f\"After removing non-null values: {len(data_cleaned)}\")\n",
    "\n",
    "# Drop the columns\n",
    "data_cleaned = data_cleaned.drop(columns=columns)\n",
    "\n",
    "print(data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.to_csv(\"../data/data-cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.describe(exclude=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocalisation Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=(45.764, 4.8357), zoom_start=12)\n",
    "\n",
    "# Sample a subset of the data (e.g., 500 points)\n",
    "sampled_data = data_cleaned.sample(n=1000, random_state=1)\n",
    "\n",
    "latitude_col = sampled_data['lat']\n",
    "longitude_col = sampled_data['long']\n",
    "\n",
    "for i in range(0, len(sampled_data)):\n",
    "\tfolium.Marker([latitude_col.iloc[i], longitude_col.iloc[i]]).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocalisation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "m = folium.Map(location=(45.764, 4.8357), zoom_start=12)\n",
    "\n",
    "# Sample a subset of the data (e.g., 500 points)\n",
    "sampled_data = data_cleaned.sample(n=1000, random_state=1)\n",
    "\n",
    "latitude_col = sampled_data['lat']\n",
    "longitude_col = sampled_data['long']\n",
    "\n",
    "heat_data = [[row['lat'], row['long']] for index, row in sampled_data.iterrows()]\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only data necessary to create clusters here would be the latitude and longitude. We'll drop all other columns as it is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to keep\n",
    "columns = ['lat', 'long']\n",
    "\n",
    "# Drop all columns except the specified ones\n",
    "df_clustering = data_cleaned[columns]\n",
    "print(df_clustering.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the longitude and latitude have comparable scales, we can use the StandardScaler to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_clustering)\n",
    "scaled_data_df = pd.DataFrame(data=scaled_data, columns=df_clustering.columns)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(scaled_data_df['lat'], scaled_data_df['long'], alpha=0.5)\n",
    "plt.title('Scaled Latitude and Longitude')\n",
    "plt.xlabel('Scaled Latitude')\n",
    "plt.ylabel('Scaled Longitude')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the same pattern observed in the heatmap is reproduced with the scaled data. This is expected, as the data has been proportionally scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find the optimal number of clusters using Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of k values to try\n",
    "k_values = range(1, 40)\n",
    "sum_square = []\n",
    "\n",
    "for k in k_values:\n",
    "\tkmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "\tkmeans.fit(scaled_data_df)\n",
    "\tsum_square.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_values, sum_square)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum square')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters can be interpreted to be between 6-10. We found that with 6-10 clusters, the granularity of places is not as well defined as the number of points are extremely vast and spreaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=70, init='k-means++')\n",
    "kmeans.fit(scaled_data_df)\n",
    "data_cleaned['cluster_kmeans'] = kmeans.labels_ \n",
    "print(data_cleaned[['cluster_kmeans', 'lat', 'long']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center map\n",
    "m = folium.Map(location=[data_cleaned['lat'].mean(), data_cleaned['long'].mean()], zoom_start=12)\n",
    "\n",
    "palette = sns.color_palette(\"hsv\", n_colors=data_cleaned['cluster_kmeans'].nunique())\n",
    "colors = [mcolors.rgb2hex(color) for color in palette]\n",
    "\n",
    "centroids = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "sampled_data = data_cleaned.sample(n=20000, random_state=1)\n",
    "cluster_counts = sampled_data['cluster_kmeans'].value_counts()\n",
    "\n",
    "valid_clusters = cluster_counts[cluster_counts >= 3].index.tolist()\n",
    "\n",
    "filtered_sampled_data = sampled_data[sampled_data['cluster_kmeans'].isin(valid_clusters)]\n",
    "cluster_id_sample = filtered_sampled_data['cluster_kmeans'].unique().tolist()\n",
    "\n",
    "print(cluster_id_sample) \n",
    "\n",
    "# Filter points based on distance to centroids\n",
    "filtered_points = filtered_sampled_data[filtered_sampled_data.apply(\n",
    "    lambda row: any(geodesic((row['lat'], row['long']), centroid).m <= 300 for centroid in centroids), axis=1)]\n",
    "\n",
    "# Map markers\n",
    "for i, row in filtered_points.iterrows():\n",
    "    point = (row['lat'], row['long'])\n",
    "    folium.CircleMarker(\n",
    "        location=point,\n",
    "        radius=1,\n",
    "        color=colors[row['cluster_kmeans']],\n",
    "        fill=True,\n",
    "        fill_color=colors[row['cluster_kmeans']]\n",
    "    ).add_to(m)\n",
    "\n",
    "# Centroids and hulls\n",
    "for cluster_id in cluster_id_sample:\n",
    "    cluster_points = filtered_points[filtered_points['cluster_kmeans'] == cluster_id][['lat', 'long']].values\n",
    "    if len(cluster_points) >= 3:  # ConvexHull requires at least 3 points\n",
    "        try: \n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=colors[cluster_id],\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Skipping cluster {cluster_id} due to QhullError\")\n",
    "    centroid = centroids[cluster_id]\n",
    "    folium.Marker(\n",
    "        location=[centroid[0], centroid[1]],\n",
    "        popup=f'Centroid {cluster_id}',\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"../html/kmeans_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H-Clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_without_duplicates = df_clustering.drop_duplicates() # Drop duplicates\n",
    "print(f\"Initial: {len(df_clustering)}\")\n",
    "print(f\"After removing duplicates: {len(df_clustering_without_duplicates)}\")\n",
    "df_clustering_without_duplicates = df_clustering_without_duplicates.sample(n=10000, random_state=1) # Sample a subset of the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data_duplicates = scaler.fit_transform(df_clustering_without_duplicates)\n",
    "scaled_data_duplicates_df = pd.DataFrame(data=scaled_data_duplicates, columns=df_clustering.columns)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(scaled_data_duplicates_df['lat'], scaled_data_duplicates_df['long'], alpha=0.5)\n",
    "plt.title('Scaled Latitude and Longitude')\n",
    "plt.xlabel('Scaled Latitude')\n",
    "plt.ylabel('Scaled Longitude')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que 'lat' et 'long' sont bien présentes dans le DataFrame\n",
    "if 'lat' not in df_clustering_without_duplicates.columns or 'long' not in df_clustering_without_duplicates.columns:\n",
    "    raise ValueError(\"Les colonnes 'lat' et 'long' doivent être présentes dans le DataFrame.\")\n",
    "\n",
    "# Si 'hierarchical_cluster' n'existe pas, on applique le clustering\n",
    "if 'hierarchical_cluster' not in df_clustering_without_duplicates.columns:\n",
    "    hierarchical_cluster = AgglomerativeClustering(n_clusters=20)  # Ajustez le nombre de clusters\n",
    "    df_clustering_without_duplicates['hierarchical_cluster'] = hierarchical_cluster.fit_predict(df_clustering_without_duplicates[['lat', 'long']])\n",
    "\n",
    "# Palette de couleurs\n",
    "palette = sns.color_palette(\"hsv\", n_colors=df_clustering_without_duplicates['hierarchical_cluster'].nunique())\n",
    "colors = [mcolors.rgb2hex(color) for color in palette]\n",
    "\n",
    "# Calculer les centroids manuellement pour chaque cluster\n",
    "centroids = df_clustering_without_duplicates.groupby('hierarchical_cluster')[['lat', 'long']].mean().values\n",
    "\n",
    "# Centrer la carte\n",
    "m = folium.Map(location=[df_clustering_without_duplicates['lat'].mean(), df_clustering_without_duplicates['long'].mean()], zoom_start=12)\n",
    "\n",
    "# Filtrer les points basés sur la distance aux centroids\n",
    "filtered_points = df_clustering_without_duplicates[df_clustering_without_duplicates.apply(\n",
    "    lambda row: any(geodesic((row['lat'], row['long']), centroid).m <= 200 for centroid in centroids), axis=1)]\n",
    "\n",
    "# Ajouter des points filtrés à la carte\n",
    "for i, row in filtered_points.iterrows():\n",
    "    point = (row['lat'], row['long'])\n",
    "    cluster_id = int(row['hierarchical_cluster'])  # Convertir en entier\n",
    "    folium.CircleMarker(\n",
    "        location=point,\n",
    "        radius=1,\n",
    "        color=colors[cluster_id],  # Utiliser 'hierarchical_cluster' pour la couleur\n",
    "        fill=True,\n",
    "        fill_color=colors[cluster_id]\n",
    "    ).add_to(m)\n",
    "\n",
    "# Dessiner les Convex Hulls et les centroids\n",
    "for cluster_id in range(df_clustering_without_duplicates['hierarchical_cluster'].nunique()):\n",
    "    cluster_points = filtered_points[filtered_points['hierarchical_cluster'] == cluster_id][['lat', 'long']].values\n",
    "    if len(cluster_points) >= 3:  # ConvexHull nécessite au moins 3 points\n",
    "        try:\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=colors[cluster_id],\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec le cluster {cluster_id}: {e}\")\n",
    "\n",
    "    centroid = centroids[cluster_id]\n",
    "    folium.Marker(\n",
    "        location=[centroid[0], centroid[1]],\n",
    "        popup=f'Centroid {cluster_id}',\n",
    "    ).add_to(m)\n",
    "\n",
    "# Sauvegarder la carte\n",
    "m.save(\"../html/hierarchical.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "def plot_k_distance_graph(X, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    distances, _ = neigh.kneighbors(X)\n",
    "    distances = np.sort(distances[:, k-1])\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel('Points')\n",
    "    plt.ylabel(f'{k}-th nearest neighbor distance')\n",
    "    plt.title('K-distance Graph')\n",
    "\n",
    "    y_min, y_max = 0, 0.1\n",
    "    y_ticks = np.arange(y_min, y_max, 0.01)  # Création des ticks de 0.001 en 0.001\n",
    "    plt.yticks(y_ticks)\n",
    "    plt.grid(axis='y', which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "# Plot k-distance graph\n",
    "plot_k_distance_graph(scaled_data_df, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.0058, min_samples=56)  \n",
    "\n",
    "dbscan.fit(scaled_data_df)\n",
    "data_cleaned['cluster_dbscan'] = dbscan.labels_\n",
    "print(data_cleaned['cluster_dbscan'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Créer une carte centrée sur la moyenne des coordonnées\n",
    "m = folium.Map(location=[data_cleaned['lat'].mean(), data_cleaned['long'].mean()], zoom_start=12)\n",
    "\n",
    "# Filtrer les clusters valides (exclure le bruit: cluster -1)\n",
    "valid_clusters = data_cleaned['cluster_dbscan'][data_cleaned['cluster_dbscan'] != -1].unique()\n",
    "palette = sns.color_palette(\"hsv\", n_colors=len(valid_clusters))\n",
    "colors = {cluster: mcolors.rgb2hex(color) for cluster, color in zip(valid_clusters, palette)}\n",
    "\n",
    "# Sous-échantillonnage\n",
    "sampled_data = data_cleaned.sample(n=min(20000, len(data_cleaned)), random_state=1)\n",
    "\n",
    "# Ajouter les points de clusters à la carte\n",
    "for i, row in sampled_data.iterrows():\n",
    "    cluster_id = row['cluster_dbscan']\n",
    "    if cluster_id == -1:  # Ignorer le bruit\n",
    "        continue\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lat'], row['long']),\n",
    "        radius=1,\n",
    "        color=colors[cluster_id],\n",
    "        fill=True,\n",
    "        fill_color=colors[cluster_id]\n",
    "    ).add_to(m)\n",
    "\n",
    "# Calculer les barycentres des clusters pour les représenter comme \"centroïdes\"\n",
    "for cluster_id in valid_clusters:\n",
    "    cluster_points = data_cleaned[data_cleaned['cluster_dbscan'] == cluster_id][['lat', 'long']].values\n",
    "    \n",
    "    if len(cluster_points) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Barycentre du cluster\n",
    "    centroid = cluster_points.mean(axis=0)\n",
    "    \n",
    "    # Ajouter un marqueur pour le barycentre\n",
    "    folium.Marker(\n",
    "        location=[centroid[0], centroid[1]],\n",
    "        popup=f'Cluster {cluster_id}',\n",
    "        icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Dessiner l'enveloppe convexe si au moins 3 points\n",
    "    if len(cluster_points) >= 3:\n",
    "        try:\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=colors[cluster_id],\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Skipping cluster {cluster_id} due to QhullError\")\n",
    "\n",
    "# Sauvegarder la carte\n",
    "m.save(\"../html/dbscan_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyproj import Proj, Transformer\n",
    "\n",
    "dbscan = DBSCAN(eps=0.02, min_samples=4)  \n",
    "dbscan.fit(scaled_data_df)\n",
    "data_cleaned['cluster_dbscan'] = dbscan.labels_\n",
    "\n",
    "\n",
    "\n",
    "# Définir la projection UTM pour la zone (ex: Lyon = UTM zone 31T)\n",
    "proj = Proj(proj=\"utm\", zone=31, ellps=\"WGS84\", datum=\"WGS84\")\n",
    "transformer = Transformer.from_proj(Proj(\"epsg:4326\"), proj)\n",
    "\n",
    "# Appliquer la transformation\n",
    "scaled_data_df['x'], scaled_data_df['y'] = transformer.transform(df_clustering['lat'], df_clustering['long'])\n",
    "\n",
    "\n",
    "# Afficher le résultat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "def plot_k_distance_graph(X, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    distances, _ = neigh.kneighbors(X)\n",
    "    distances = np.sort(distances[:, k-1])\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel('Points')\n",
    "    plt.ylabel(f'{k}-th nearest neighbor distance')\n",
    "    plt.title('K-distance Graph')\n",
    "\n",
    "    y_min, y_max = 0, 100\n",
    "    y_ticks = np.arange(y_min, y_max, 10)  # Création des ticks de 0.001 en 0.001\n",
    "    plt.yticks(y_ticks)\n",
    "    plt.grid(axis='y', which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "# Plot k-distance graph\n",
    "plot_k_distance_graph(scaled_data_df, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
